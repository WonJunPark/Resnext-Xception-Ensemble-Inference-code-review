{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WonJunPark/Resnext-Xception-Ensemble-Inference-code-review/blob/master/Resnext-Xception-Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl_x3eT6NBin",
        "colab_type": "text"
      },
      "source": [
        "Resnext & Xception Ensemble (Inference)\n",
        "\n",
        "\n",
        "This kernel outputs the ensemble of the results from https://www.kaggle.com/\n",
        "\n",
        "1.   khoongweihao/frames-per-video-viz and https://www.kaggle.com/greatgamedota/xception-binary-classifier-inference (not original, modified learning rate and epochs)\n",
        "2.   Frames per video at 64 (best found)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPmE4yMnOiK4",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Def0fM4ZNBs_",
        "colab_type": "code",
        "outputId": "e1a6bd69-a773-469a-9443-df1323f5e04a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDeU0VYfNbIe",
        "colab_type": "code",
        "outputId": "ad2cf7cd-60d1-4e16-853c-059fe021bb4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd /content/gdrive/My Drive/deepfake-detection-challenge"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/deepfake-detection-challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B25z2i1FN5Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = \"/content/gdrive/My Drive/deepfake-detection-challenge/test_videos/\"\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sd162txN5YS",
        "colab_type": "code",
        "outputId": "29b33612-583b-46b4-d788-28ed05d2fcc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "frame_h = 5\n",
        "frame_l = 5\n",
        "len(test_videos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPiE_vXjOp_q",
        "colab_type": "text"
      },
      "source": [
        "# 버전 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY5S1IPZOMRJ",
        "colab_type": "code",
        "outputId": "8746e91e-8a53-4abd-aa54-dd3da23e9247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"cuDNN version:\", torch.backends.cudnn.version())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.4.0\n",
            "CUDA version: 10.1\n",
            "cuDNN version: 7603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeIE18zwOMTn",
        "colab_type": "code",
        "outputId": "85c8e80e-1238-421b-a7ce-b608f1fc7a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-VE0nLUOuOg",
        "colab_type": "text"
      },
      "source": [
        "# Resnext model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_yTYQ7rOMWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
        "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tln9on2rPYmk",
        "colab_type": "text"
      },
      "source": [
        "Blazeface\n",
        "- google reserach의 빠르고 가벼운 얼굴 탐지기\n",
        "- 눈2 귀2 코1 입1에 대한 6개의 키포인트를 예측\n",
        "- https://github.com/hollance/BlazeFace-PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae5Ts7oTOZoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from blazeface import BlazeFace\n",
        "facedet = BlazeFace().to(gpu)\n",
        "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\") # blazeface.pth : 훈련 된 모델의 가중치\n",
        "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\") # anchors.npy : 앵커 박스가있는 룩업 테이블\n",
        "_ = facedet.train(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-s_0nvvU8Ya",
        "colab_type": "text"
      },
      "source": [
        "VideoReader\n",
        "- 파이썬에서 비디오 파일 읽기를 단순화\n",
        "- https://github.com/postpop/videoreader\n",
        "\n",
        "FaceExtractor\n",
        "- 얼굴 추출기는 비디오에서 얼굴 데이터 세트를 만드는 도구입니다. 입력 데이터 (사진 또는 비디오)를 읽고 그 안에있는 얼굴을 검색하고 얼굴을 찾으면 자르고 원본 입력 파일을 수정하지 않고 출력 폴더에 인쇄합니다.\n",
        "- https://github.com/Baycosinus/face-extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_A5EPrfR_yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from helpers.read_video_1 import VideoReader\n",
        "from helpers.face_extract_1 import FaceExtractor\n",
        "\n",
        "frames_per_video = 64 #frame_h * frame_l\n",
        "video_reader = VideoReader() # 비디오를 읽음\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video) #비디오에서 얼굴 추출\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY3w2Km7VtOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N09Vcr5lVqSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.transforms import Normalize #정규화\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ehji7p1Vqdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95meolLZVzXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models #resnet model 불러오기\n",
        "\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3], \n",
        "                                        groups=32, \n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE1EfjwhVzaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu) # 사전학습 모델\n",
        "\n",
        "model = MyResNeXt().to(gpu)\n",
        "model.load_state_dict(checkpoint) #학습해둔 모델을 불러옴\n",
        "_ = model.eval()\n",
        "\n",
        "del checkpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1NwgZt0Vz-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path) # 비디오에서 얼굴을 추출\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces) #한개의 얼굴에 프레임을 씌움\n",
        "        \n",
        "        if len(faces) > 0:\n",
        "            # NOTE: When running on the CPU, the batch size must be fixed\n",
        "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8) #one-hot 인코딩\n",
        "\n",
        "            n = 0\n",
        "            for frame_data in faces: # 1개의 mp4 파일\n",
        "                for face in frame_data[\"faces\"]: # mp4파일 내의 얼굴 이미지들\n",
        "                    # Resize to the model's required input size.\n",
        "                    # We keep the aspect ratio intact and add zero\n",
        "                    # padding if necessary.                    \n",
        "                    resized_face = isotropically_resize_image(face, input_size) # input_size에 맞게 이미지 전처리 (input_size = 224)\n",
        "                    resized_face = make_square_image(resized_face)  # 얼굴 부분에 squear을 만듬\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
        "                    \n",
        "                    # Test time augmentation: horizontal flips.\n",
        "                    # TODO: not sure yet if this helps or not\n",
        "                    #x[n] = cv2.flip(resized_face, 1)\n",
        "                    #n += 1\n",
        "\n",
        "            if n > 0: #전처리 완료된 이미지들을 모델에 대입\n",
        "                x = torch.tensor(x, device=gpu).float()\n",
        "\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.) #정규화\n",
        "\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(x) # resnext 모델로 예측\n",
        "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                    return y_pred[:n].mean().item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WVWGIFViBu4",
        "colab_type": "text"
      },
      "source": [
        "concurrent.futures \n",
        "- 병렬 작업 실행\n",
        "- https://docs.python.org/ko/3.7/library/concurrent.futures.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faFqCYxpV0BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def predict_on_video_set(videos, num_workers):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video) #모든 test set에 대한 예측값\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos))) #병렬 작업\n",
        "\n",
        "    return list(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFyPpdlEX3TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "speed_test = False  #속도를 확인하려면 True값으로 지정 # you have to enable this manually"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qc1A5_X3Vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if speed_test:\n",
        "    start_time = time.time()\n",
        "    speedtest_videos = test_videos[:5]\n",
        "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxoVWYNsX3YM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = predict_on_video_set(test_videos, num_workers=4) #병렬 처리 시작"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7obXr5MX7OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df_resnext = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
        "submission_df_resnext.to_csv(\"submission_resnext.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_orJROukedj",
        "colab_type": "text"
      },
      "source": [
        "# Xception model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZk0T12kkhzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6f86349a-0189-475c-fd21-2f817c3cf14a"
      },
      "source": [
        "# 설치\n",
        "!pip install ../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Requirement '../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl' looks like a filename, but the file does not exist\u001b[0m\n",
            "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl'\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-uQ2_V2kh3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터 불러오기, resnext와 동일\n",
        "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
        "\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "len(test_videos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfoauaxrk55I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnext와 동일\n",
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZACNOrvk57w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnext와 동일\n",
        "import sys\n",
        "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
        "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvhn0qoNk5-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 영상 전처리, resnext와 동일\n",
        "from blazeface import BlazeFace\n",
        "facedet = BlazeFace().to(gpu)\n",
        "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
        "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
        "_ = facedet.train(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0p8wI4vk6CZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 영상 전처리, resnext와 동일\n",
        "from helpers.read_video_1 import VideoReader\n",
        "from helpers.face_extract_1 import FaceExtractor\n",
        "\n",
        "frames_per_video = 64 # originally 4\n",
        "\n",
        "video_reader = VideoReader()\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC3Bgjcpkh6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnext에서는 input_size를 224로 줬는데, xception net은 150으로 줌 (?)\n",
        "input_size = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXZbWHq3kh87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정규화, resnext와 동일\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOLED2fukh1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Face extraction, resnext와 동일\n",
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy5FU4w3lnFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls ../input/deepfake-xception-trained-model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DHgonJ5lnKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ★ 제일 중요한 부분인데, 토치 공부가 미흡해서 해석이 힘듬..\n",
        "from pytorchcv.model_provider import get_model\n",
        "model = get_model(\"xception\", pretrained=False)\n",
        "model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n",
        "\n",
        "class Pooling(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pooling, self).__init__()\n",
        "    \n",
        "    self.p1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.p2 = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.p1(x)\n",
        "    x2 = self.p2(x)\n",
        "    return (x1+x2) * 0.5\n",
        "\n",
        "model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)))\n",
        "\n",
        "class Head(torch.nn.Module):\n",
        "  def __init__(self, in_f, out_f):\n",
        "    super(Head, self).__init__()\n",
        "    \n",
        "    self.f = nn.Flatten()\n",
        "    self.l = nn.Linear(in_f, 512)\n",
        "    self.d = nn.Dropout(0.5)\n",
        "    self.o = nn.Linear(512, out_f)\n",
        "    self.b1 = nn.BatchNorm1d(in_f)\n",
        "    self.b2 = nn.BatchNorm1d(512)\n",
        "    self.r = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.f(x)\n",
        "    x = self.b1(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    x = self.l(x)\n",
        "    x = self.r(x)\n",
        "    x = self.b2(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    out = self.o(x)\n",
        "    return out\n",
        "\n",
        "class FCN(torch.nn.Module):\n",
        "  def __init__(self, base, in_f):\n",
        "    super(FCN, self).__init__()\n",
        "    self.base = base\n",
        "    self.h1 = Head(in_f, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.base(x)\n",
        "    return self.h1(x)\n",
        "\n",
        "net = []\n",
        "model = FCN(model, 2048)\n",
        "model = model.cuda()\n",
        "model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth')) # new, updated\n",
        "net.append(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5SZpAwwlnMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 예측, resnext와 동일\n",
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces)\n",
        "        \n",
        "        if len(faces) > 0:\n",
        "            # NOTE: When running on the CPU, the batch size must be fixed\n",
        "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "\n",
        "            # If we found any faces, prepare them for the model.\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    # Resize to the model's required input size.\n",
        "                    # We keep the aspect ratio intact and add zero\n",
        "                    # padding if necessary.                    \n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = make_square_image(resized_face)\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
        "                    \n",
        "                    # Test time augmentation: horizontal flips.\n",
        "                    # TODO: not sure yet if this helps or not\n",
        "                    #x[n] = cv2.flip(resized_face, 1)\n",
        "                    #n += 1\n",
        "\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=gpu).float()\n",
        "\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "                    # x[i] = x[i] / 255.\n",
        "\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(x)\n",
        "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                    return y_pred[:n].mean().item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZbCkAwQlnIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkJlIWXrm1Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 병렬 처리, resnext와 동일\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def predict_on_video_set(videos, num_workers):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "\n",
        "    return list(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKIa7wDPm1Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 속도 확인, resnext와 동일\n",
        "speed_test = False\n",
        "\n",
        "if speed_test:\n",
        "    start_time = time.time()\n",
        "    speedtest_videos = test_videos[:5]\n",
        "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njuoGoJLm1a5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "model.eval()\n",
        "predictions = predict_on_video_set(test_videos, num_workers=4) #전체 mp4 파일 예측 시간 확인"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpMRLdCDm1WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xception 모델 예측\n",
        "submission_df_xception = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
        "submission_df_xception.to_csv(\"submission_xception.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HRrRDnYnUgf",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble of Resnext and Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NhY4CbSnQDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnext 결과\n",
        "submission_df_resnext.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKYFjHZonPLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# xception 결과\n",
        "submission_df_xception.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66NleiWncfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.DataFrame({\"filename\": test_videos})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otk4ab65nch1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r1 = 0.46441 #xception 비율\n",
        "r2 = 0.52189 # resnenext 비율\n",
        "total = r1 + r2\n",
        "r11 = r1/total\n",
        "r22 = r2/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5fCX9qtnck2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df[\"label\"] = r22*submission_df_resnext[\"label\"] + r11*submission_df_xception[\"label\"] #앙상블"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II2O7m-ong4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}